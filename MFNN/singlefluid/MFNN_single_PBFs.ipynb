{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单流体嵌入网络的PBFs聚合物"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random, genfromtxt\n",
    "from IPython.display import display\n",
    "from matplotlib import rc\n",
    "from matplotlib.pyplot import figure\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.ticker as mticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据类型\n",
    "DTYPE = torch.float32\n",
    "\n",
    "# 设置随机种子\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "df = {}\n",
    "optionM=2 # 0:训练储存模量；1：训练损耗模量\n",
    "# 读取数据\n",
    "# url_lf = 'Oscillatory/Data_LF_AS_Gp.xlsx'\n",
    "url_hf = 'Data_HF.xlsx'\n",
    "\n",
    "# df_LF = pd.read_excel(url_lf, sheet_name=None)\n",
    "# data_LF = [[k, v] for k, v in df_LF.items()]  # k is the sheet name, v is the pandas df\n",
    "\n",
    "df_HF = pd.read_excel(url_hf, sheet_name=None)\n",
    "data_HF = [[k, v] for k, v in df_HF.items()]  # k is the sheet name, v is the pandas df\n",
    "\n",
    "sample = 1\n",
    "data_HF[sample][1] = data_HF[sample][1].dropna()\n",
    "\n",
    "df_hf = data_HF[sample][1]  # entire data\n",
    "\n",
    "data_HF[sample][1] = data_HF[sample][1][data_HF[sample][1].DP != 161]\n",
    "\n",
    "# 划分验证集\n",
    "data_valid=df_hf[df_hf.DP == 162.]\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "#x1_d_LF = torch.tensor(data_LF[sample][1]['Strain'].values, dtype=torch.float32).view(-1, 1)\n",
    "#x2_d_LF = torch.tensor(data_LF[sample][1]['Temperature'].values, dtype=torch.float32).view(-1, 1)\n",
    "#x3_d_LF = torch.tensor(data_LF[sample][1]['AngFreq'].values, dtype=torch.float32).view(-1, 1)\n",
    "#y1_d_LF = torch.tensor(data_LF[sample][1]['StorageM'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "x1_d_HF = torch.tensor(data_HF[sample][1]['DP'].values, dtype=torch.float32).view(-1, 1)\n",
    "x2_d_HF = torch.tensor(data_HF[sample][1]['Mn'].values, dtype=torch.float32).view(-1, 1)\n",
    "x3_d_HF = torch.tensor(data_HF[sample][1]['PDI'].values, dtype=torch.float32).view(-1, 1)\n",
    "x4_d_HF = torch.tensor(data_HF[sample][1]['AngFreq'].values, dtype=torch.float32).view(-1, 1)\n",
    "y1_d_HF = torch.tensor(data_HF[sample][1]['LossFactor'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# 对 HF 数据进行对数变换\n",
    "x4_d_HF = torch.log10(x4_d_HF) # 频率对数化\n",
    "#y1_d_HF = torch.log10(y1_d_HF) # 损耗角对数化\n",
    "\n",
    "# 计算边界\n",
    "\n",
    "x1min, x1max = torch.min(x1_d_HF), torch.max(x1_d_HF)\n",
    "x2min, x2max = torch.min(x2_d_HF), torch.max(x2_d_HF)\n",
    "x3min, x3max = torch.min(x3_d_HF), torch.max(x3_d_HF)\n",
    "x4min, x4max = torch.min(x4_d_HF), torch.max(x4_d_HF)\n",
    "y1min, y1max = torch.min(y1_d_HF), torch.max(y1_d_HF)\n",
    "# 索引3,4,5为对数化\n",
    "lb = torch.tensor([x1min, x2min, x3min, x4min,y1min], dtype=torch.float32).numpy()\n",
    "ub = torch.tensor([x1max, x2max, x3max, x4max,y1max], dtype=torch.float32).numpy()\n",
    "# 归一化函数\n",
    "def norm(vectors, lb, ub):\n",
    "    normalized_vectors = (vectors - lb) / (ub - lb)\n",
    "    return normalized_vectors\n",
    "\n",
    "x1_d_HF = norm(x1_d_HF, lb[0], ub[0])\n",
    "x2_d_HF = norm(x2_d_HF, lb[1], ub[1])\n",
    "x3_d_HF = norm(x3_d_HF, lb[2], ub[2])\n",
    "x4_d_HF = norm(x4_d_HF, lb[3], ub[3])\n",
    "y1_d_HF = norm(y1_d_HF, lb[4], ub[4])\n",
    "# 合并数据\n",
    "X_data_HF = torch.cat([x1_d_HF, x2_d_HF, x3_d_HF,x4_d_HF], dim=1)\n",
    "y_data_HF= torch.cat([y1_d_HF], dim=1) # 默认训练储存模量\n",
    "#X_data_LF = torch.cat([x1_d_LF, x2_d_LF, x3_d_LF], dim=1)\n",
    "#y_data_LF = torch.cat([y1_d_LF], dim=1)\n",
    "\n",
    "Xy_data_HF = torch.cat([X_data_HF,y_data_HF], dim=1)\n",
    "#Xy_data_LF = torch.cat([x1_d_LF, x2_d_LF, x3_d_LF, y1_d_LF], dim=1)\n",
    "\n",
    "Shuffle = False # 是否打乱训练\n",
    "if Shuffle:\n",
    "    Xy_data_HF = Xy_data_HF[torch.randperm(Xy_data_HF.size(0))]\n",
    "    #Xy_data_LF = Xy_data_LF[torch.randperm(Xy_data_LF.size(0))]\n",
    "    X_data_HF = Xy_data_HF[:, 0:4]\n",
    "    y_data_HF = Xy_data_HF[:, 4:5]\n",
    "    #X_data_LF = Xy_data_LF[:, 0:3]\n",
    "    #y_data_LF = Xy_data_LF[:, 3:4]\n",
    "\n",
    "# 定义模型\n",
    "in_dim, out_dim = 4, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_d_valid = torch.tensor(data_valid['DP'].values, dtype=torch.float32).view(-1, 1)\n",
    "x2_d_valid = torch.tensor(data_valid['Mn'].values, dtype=torch.float32).view(-1, 1)\n",
    "x3_d_valid = torch.tensor(data_valid['PDI'].values, dtype=torch.float32).view(-1, 1)\n",
    "x4_d_valid = torch.tensor(data_valid['AngFreq'].values, dtype=torch.float32).view(-1, 1)\n",
    "y1_d_valid = torch.tensor(data_valid['LossFactor'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# 对 HF 数据进行对数变换\n",
    "x4_d_valid = torch.log10(x4_d_valid) # 频率对数化\n",
    "#y1_d_valid = torch.log10(y1_d_valid) # 损耗因子对数化\n",
    "\n",
    "x1_d_valid = norm(x1_d_valid, lb[0], ub[0])\n",
    "x2_d_valid = norm(x2_d_valid, lb[1], ub[1])\n",
    "x3_d_valid = norm(x3_d_valid, lb[2], ub[2])\n",
    "x4_d_valid = norm(x4_d_valid, lb[3], ub[3])\n",
    "y1_d_valid = norm(y1_d_valid, lb[4], ub[4])\n",
    "\n",
    "# 合并数据\n",
    "X_data_valid = torch.cat([x1_d_valid, x2_d_valid, x3_d_valid,x4_d_valid], dim=1)\n",
    "y_data_valid= torch.cat([y1_d_valid], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络的类\n",
    "class PINN_NeuralNet(nn.Module):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim=0,\n",
    "                 output_dim=1,  # 默认输出维度为1\n",
    "                 num_hidden_layers=4, \n",
    "                 num_neurons_per_layer=20,\n",
    "                 activation='tanh',\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        super(PINN_NeuralNet, self).__init__()\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # 添加输入层\n",
    "        self.input_layer = nn.Linear(input_dim, num_neurons_per_layer)\n",
    "        \n",
    "        # 添加其他隐藏层\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.hidden_layers.append(nn.Linear(num_neurons_per_layer, num_neurons_per_layer))\n",
    "        \n",
    "        # 添加输出层\n",
    "        self.out = nn.Linear(num_neurons_per_layer, output_dim)\n",
    "        # 设置激活函数\n",
    "        if activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'relu':\n",
    "            self.activation = F.relu6\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "        elif activation == 'linear':\n",
    "            self.activation = None\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        \n",
    "        # 初始化权重\n",
    "        if kernel_initializer == 'glorot_normal':\n",
    "            nn.init.xavier_normal_(self.input_layer.weight)\n",
    "            for hidden_layer in self.hidden_layers:\n",
    "                nn.init.xavier_normal_(hidden_layer.weight)\n",
    "            nn.init.xavier_normal_(self.out.weight)\n",
    "        elif kernel_initializer == 'glorot_uniform':\n",
    "            nn.init.xavier_uniform_(self.input_layer.weight)\n",
    "            for hidden_layer in self.hidden_layers:\n",
    "                nn.init.xavier_uniform_(hidden_layer.weight)\n",
    "            nn.init.xavier_uniform_(self.out.weight)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported kernel initializer\")\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # 进入输入层\n",
    "        Z = self.input_layer(X)\n",
    "        \n",
    "        # 通过隐藏层\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            Z = hidden_layer(Z)\n",
    "            if self.activation is not None:\n",
    "                Z = self.activation(Z)\n",
    "        # 通过输出层输出\n",
    "        Z = self.out(Z)\n",
    "        \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络处理器类\n",
    "class PINNSolver():\n",
    "\n",
    "    # 类属性定义\n",
    "    def __init__(self, model_HF_nl, model_HF_l):\n",
    "        # self.model_LF = model_LF # 低保真模型\n",
    "        self.model_HF_nl = model_HF_nl # 高保真非线性模型\n",
    "        self.model_HF_l = model_HF_l # 高保真线性模型\n",
    "        # Initialize history of losses and global iteration counter\n",
    "        self.hist =  [[], []] # loss历史列表,0:train loss ;1:valid loss\n",
    "        self.iter = 0 # 迭代次数\n",
    "        self.last_n_losses = [] # 前损失列表\n",
    "\n",
    "    # 更新损失列表   \n",
    "    def update_last_n_losses(self, loss):\n",
    "        self.last_n_losses.append(loss)\n",
    "        if len(self.last_n_losses) > 20:\n",
    "            self.last_n_losses.pop(0)\n",
    "\n",
    "    # 计算最大相对误差        \n",
    "    def ES(self):\n",
    "        if len(self.last_n_losses) < 20:\n",
    "            return 100  # a large number\n",
    "\n",
    "        current_loss = self.last_n_losses[-1]\n",
    "        max_relative_error = 100.*max([abs(current_loss - loss) / current_loss for loss in self.last_n_losses[:-1]])\n",
    "        return max_relative_error\n",
    "    \n",
    "    # 计算loss，模型核心\n",
    "    def loss_fn(self, X_data_HF, y_data_HF,X_data_valid,y_data_valid):\n",
    "        y_pred_valid_nl = self.model_HF_nl(X_data_valid)\n",
    "\n",
    "        y_pred_valid_l = self.model_HF_l(X_data_valid)\n",
    "\n",
    "        y_pred_valid = y_pred_valid_nl + y_pred_valid_l       \n",
    "        #y_pred_LF = self.model_LF(X_data_LF)\n",
    "\n",
    "       # y_pred_LF_HF = self.model_LF(X_data_HF)\n",
    "   \n",
    "        y_pred_HF_nl = self.model_HF_nl(X_data_HF)\n",
    "\n",
    "        y_pred_HF_l = self.model_HF_l(X_data_HF)\n",
    "\n",
    "        y_pred_HF = y_pred_HF_nl + y_pred_HF_l\n",
    "\n",
    "        Loss_L2 = 1e-5 * sum(torch.sum(w_**2) for w_ in self.model_HF_nl.parameters())\n",
    "        Loss_L2 += 1e-5 * sum(torch.sum(w_**2) for w_ in self.model_HF_l.parameters())\n",
    "        #Loss_data_LF = torch.mean((y_data_LF - y_pred_LF)**2)\n",
    "\n",
    "        Loss_data_HF = torch.mean((y_data_HF - y_pred_HF)**2)+Loss_L2\n",
    "        Loss_data_valid=torch.mean((y_pred_valid-y_data_valid)**2)+Loss_L2\n",
    "                \n",
    "        return Loss_data_HF,Loss_data_valid\n",
    "    # 训练核心函数，包括loss计算梯度计算和反向传播\n",
    "    def solve_with_PyTorch_optimizer(self, optimizer,data,scheduler,N=1001):\n",
    "        \"\"\"This method performs a gradient descent type optimization.\"\"\"        \n",
    "        for i in range(N):\n",
    "            # 梯度清0\n",
    "            optimizer.zero_grad()\n",
    "            # 计算loss          \n",
    "            loss,loss_valid = self.loss_fn(data[0], data[1],data[2],data[3])\n",
    "            # 反向传播计算梯度\n",
    "            loss.backward()\n",
    "            # 根据loss调度学习率\n",
    "            scheduler.step(loss)\n",
    "            # 反向传播更新权重和偏置\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录loss并计算相对误差\n",
    "            self.current_loss = loss.item()\n",
    "            self.valid_loss=loss_valid.item()\n",
    "           \n",
    "            self.max_relative_error = self.ES()\n",
    "            self.callback(self.max_relative_error,N)  # Pass max_relative_error to the callback function\n",
    "            self.update_last_n_losses(self.current_loss)\n",
    "\n",
    "            # 早停机制\n",
    "            if self.max_relative_error < 2e-3: # in %\n",
    "                print('Early stopping... \\nIt {:05,d}: Loss = {:10.4e}, Max. rel. error = {} %'.format(self.iter,\n",
    "                                                             self.current_loss,\n",
    "                                                            np.round(self.max_relative_error, 3)))\n",
    "                break\n",
    "\n",
    "    # 打印loss    \n",
    "    def callback(self, xr=None,N=1001):\n",
    "        if self.iter % 1000 == 0:\n",
    "            print('It {:05,d}: Loss = {:10.4e}, Max. rel. error = {} %'.format(self.iter,\n",
    "                                                             self.current_loss,\n",
    "                                                            np.round(self.max_relative_error, 2)))\n",
    "        self.hist[0].append(self.current_loss)\n",
    "        self.hist[1].append(self.valid_loss)\n",
    "        self.iter+=1\n",
    "    \n",
    "    def plot_loss_history(self, ax=None):\n",
    "        if not ax:\n",
    "            fig = plt.figure(figsize=(7, 5))\n",
    "            ax = fig.add_subplot(111)\n",
    "\n",
    "        # 绘制训练集损失曲线\n",
    "        ax.semilogy(range(len(self.hist[0])), self.hist[0], 'b-', label='Training Loss')\n",
    "        \n",
    "        # 绘制验证集损失曲线\n",
    "        ax.semilogy(range(len(self.hist[1])), self.hist[1], 'g-', label='Validation Loss')\n",
    "\n",
    "        ax.set_xlabel('$n_{epoch}$')\n",
    "        ax.set_ylabel('$loss$')\n",
    "        ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, pos: f'{int(x):,}'))\n",
    "        ax.legend()  # 添加图例\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "#model_LF = PINN_NeuralNet(input_dim=in_dim,\n",
    "#                          output_dim=out_dim,\n",
    "#                          num_hidden_layers=4,\n",
    "#                          num_neurons_per_layer=64,\n",
    "#                          activation='tanh'\n",
    "#                          )\n",
    "model_HF_nl = PINN_NeuralNet(input_dim=in_dim,\n",
    "                             output_dim=out_dim,\n",
    "                             num_hidden_layers=4,\n",
    "                             num_neurons_per_layer=64,\n",
    "                             activation='relu')\n",
    "model_HF_l = PINN_NeuralNet(input_dim=in_dim,\n",
    "                            output_dim=out_dim,\n",
    "                            num_hidden_layers=1,\n",
    "                            num_neurons_per_layer=10,\n",
    "                            activation='linear')\n",
    "\n",
    "# 初始化 PINNSolver\n",
    "solver = PINNSolver(model_HF_nl, model_HF_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义学习率调度器\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(list(model_HF_nl.parameters()) + list(model_HF_l.parameters()), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=10, verbose=True)\n",
    "# 定义训练模式\n",
    "mode = 'PyTorch_optimizer'\n",
    "N = int(1000) + 1  # 训练迭代次数\n",
    "\n",
    "try:\n",
    "    runtime\n",
    "except NameError:\n",
    "    runtime = 0.\n",
    "\n",
    "if mode == 'PyTorch_optimizer':\n",
    "    try:\n",
    "        t0 = time()\n",
    "        solver.solve_with_PyTorch_optimizer(optimizer, [X_data_HF,y_data_HF,X_data_valid,y_data_valid],scheduler,N=N)\n",
    "        runtime += (time() - t0) / 60.\n",
    "        print('\\nRuntime: {:.3f} minutes'.format(runtime))\n",
    "    except KeyboardInterrupt:\n",
    "        runtime += (time() - t0) / 60.\n",
    "        print('\\nRuntime: {:.3f} minutes'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=18)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.rc('figure', titlesize=18)\n",
    "colors = ['tab:red', 'tab:orange', '#f9c74f', 'tab:green', 'tab:cyan', 'tab:blue', 'tab:purple', 'tab:brown', 'tab:pink']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\n",
    "def denorm(normalized_vectors, lb, ub):\n",
    "    original_vectors = normalized_vectors * (ub - lb) + lb\n",
    "    return original_vectors\n",
    "\n",
    "N_exp = 1000\n",
    "# 构建1000个绘图的频率点，3为对数化后的上下限,所以这里的w_plot的点都是对数化后的\n",
    "w_plot = np.linspace(lb[3], ub[3], N_exp).reshape(-1, 1)\n",
    "# 这里是要画不同DP的曲线\n",
    "DP_range = [40., 162., 274., 454., 611.,798.,1014.,1184.,1524.]\n",
    "\n",
    "for i in range(len(DP_range)):\n",
    "    test_data=df_hf[df_hf.DP==DP_range[i]]\n",
    "    #  x1_test和y1_test是用来绘图的，不参与神经网络计算，相当于测试集\n",
    "    x1_test=test_data['AngFreq'].values # 未对数化、未归一化,np\n",
    "    y1_test = test_data['LossFactor'].values # 未对数化，未归一化, np\n",
    "    \n",
    "    DP_plot=norm(DP_range[i]*np.ones(N_exp).reshape(-1, 1),lb[0],ub[0])\n",
    "    \n",
    "    Mn_plot=norm(np.unique(test_data['Mn'])*np.ones(N_exp).reshape(-1, 1),lb[1],ub[1])\n",
    "    PDI_plot=norm(np.unique(test_data['PDI'])*np.ones(N_exp).reshape(-1, 1),lb[2],ub[2])\n",
    "    \n",
    "    # 组合成一个大的 NumPy 数组\n",
    "    X_MF_np = np.hstack((DP_plot, Mn_plot, PDI_plot, norm(w_plot,lb[3],ub[3])))\n",
    "    \n",
    "    # 将 NumPy 数组转换为 PyTorch 张量\n",
    "    X_MF = torch.tensor(X_MF_np, dtype=torch.float32)\n",
    "    y_MF = model_HF_nl(X_MF) + model_HF_l(X_MF)\n",
    "    # 反归一化 y_MF\n",
    "    y_MF_denorm = denorm(y_MF.detach().numpy(), lb[4], ub[4])\n",
    "     # 绘制数据\n",
    "   \n",
    "    ax.plot(10**w_plot, y_MF_denorm, color=colors[i % len(colors)])\n",
    "   \n",
    "    i # 根据条件设置散点图的标记\n",
    "    if i == 1:\n",
    "        ax.scatter(x1_test, y1_test, color=colors[i % len(colors)], marker='o', facecolors='none', label=f'DP = {DP_range[i]}\\n(Test)',s=10)\n",
    "    else:\n",
    "       ax.scatter(x1_test, y1_test, color=colors[i % len(colors)], marker='o', label=f'DP = {DP_range[i]}',s=10)\n",
    "       \n",
    "    \n",
    "    \n",
    "# ax.axvspan(1, 100, color='gray', alpha=0.2) 突出标记\n",
    "ax.set_xscale('log')\n",
    "#ax.set_yscale('log')\n",
    "ax.set_ylabel('$tan\\delta$')\n",
    "ax.set_xlabel('$\\omega$ $\\mathrm{[rad/s]}$')\n",
    "# ax.grid()\n",
    "# 调整图例的大小和位置\n",
    "legend = ax.legend(fontsize=7.5, loc='lower right', bbox_to_anchor=(0.4, 0.54))\n",
    "#ax.invert_xaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型的状态字典\n",
    "torch.save(model_HF_nl.state_dict(), 'model/model_nl_pinn_pbfs_lossf.pth')\n",
    "torch.save(model_HF_l.state_dict(), 'model/model_l_pinn_pbfs_lossf.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平滑数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "import pandas as pd\n",
    "# 数据\n",
    "b1_values = [\n",
    "    100000, 63100, 39800, 25100, 15800, 10000, 6310, 3980, 2510, 1580, \n",
    "    1000, 631, 398, 251, 158, 100, 63.1, 39.8, 25.1, 15.8, \n",
    "    10, 6.31, 3.98, 2.51, 1.58, 1, 0.631, 0.398, 0.251, 0.158, \n",
    "    0.1, 0.0631, 0.0398, 0.0251, 0.0158, 0.01\n",
    "]\n",
    "\n",
    "new_values = [\n",
    "    1.65035, 1.5205, 1.4306, 1.3392, 1.2508, 1.19528, 1.17055, 1.144, 1.119,\n",
    "    1.075, 1.053, 1.031, 1.01, 0.99, 1.00, 1.02, 1.03, 1.04007,\n",
    "    1.04582, 1.0522, 1.05732, 1.05929, 1.0637, 1.05992, 1.04606, 1.02023, 0.98621,\n",
    "    0.9377, 0.88793, 0.83564, 0.78398, 0.73421, 0.68696, 0.64412, 0.6101, 0.58364\n",
    "]\n",
    "\n",
    "# 对数据进行平滑处理\n",
    "window_length = 11  # 窗口长度（必须是奇数）\n",
    "polyorder = 3       # 多项式拟合的阶数\n",
    "smoothed_values = savgol_filter(new_values, window_length, polyorder)\n",
    "# 创建图表\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 绘制原始数据\n",
    "#plt.semilogx(b1_values, new_values, marker='s', linestyle='--', color='g', label='Original Data')\n",
    "\n",
    "# 绘制平滑后的数据\n",
    "plt.semilogx(b1_values, smoothed_values, marker='o', linestyle='-', color='b', label='Smoothed Data')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('B1 vs Corresponding Values (Smoothed)')\n",
    "plt.xlabel('B1 Values (Log Scale)')\n",
    "plt.ylabel('Corresponding Values')\n",
    "\n",
    "# 添加网格\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n",
    "# 将数据保存到 DataFrame\n",
    "data = {\n",
    "    'B1 Values': b1_values,\n",
    "    'Original Values': new_values,\n",
    "    'Smoothed Values': smoothed_values\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 导出到 Excel 文件\n",
    "output_file = 'smoothed_data.xlsx'\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"数据已导出到 {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PINN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
